# ğŸ¤– Celeby Agentic RAG

**Production-ready self-correcting RAG system with LangGraph orchestration, hybrid search, and intelligent re-ranking**

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![React](https://img.shields.io/badge/React-18+-61DAFB.svg)](https://react.dev/)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED.svg)](https://www.docker.com/)
[![License](https://img.shields.io/badge/License-Apache%202.0-yellow.svg)](LICENSE)

---

## ğŸ¯ What Makes This Special?

Unlike traditional RAG systems, **Celeby Agentic RAG is an autonomous agent** that validates and self-corrects its responses:

```mermaid
graph LR
    A[Query] --> B[Multi-Query Rewrite]
    B --> C[Hybrid Search]
    C --> D[Re-rank]
    D --> E[Generate]
    E --> F{Validate}
    F -->|Invalid| B
    F -->|Valid| G[Return]
```

**Key Innovation**: Self-correction loop with validation gates achieves **11.1% automatic answer improvement** without human intervention.

### Why Celeby Agentic RAG?

| Problem | Traditional RAG | Celeby Agentic RAG |
|---------|----------------|-------------------|
| **Hallucinations** | No validation | âœ… Validates every answer, retries if unsupported |
| **Poor Retrieval** | Single strategy | âœ… Hybrid search + query rewriting |
| **No Quality Metrics** | Blind to accuracy | âœ… Built-in RAGAS evaluation |
| **Slow Responses** | No optimization | âœ… Redis caching + streaming |

---

## ğŸš€ Quick Start

### Prerequisites

- **Docker & Docker Compose** (20.10+)
- **8GB+ RAM** (16GB optimal)
- **~10GB disk space** for models
- **NVIDIA GPU** (optional, for faster inference)

### Installation

#### 1. Clone Repository

```bash
git clone https://github.com/merenceleby/Celeby-Agentic-RAG.git
cd Celeby-Agentic-RAG
```

#### 2. Start Services (Choose Your Hardware)

**CPU-only:**
```bash
docker-compose --profile cpu up -d
```

**NVIDIA GPU (Tested on RTX series):**
```bash
# Verify GPU access first
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# Start services
docker-compose --profile nvidia up -d
```

**AMD GPU:**
```bash
# Verify ROCm
docker run --rm --device=/dev/kfd --device=/dev/dri rocm/pytorch:latest rocm-smi

# Start services
docker-compose --profile amd up -d
```

**Apple Silicon (M1/M2/M3):**
```bash
docker-compose --profile mac up -d
```

#### 3. Pull Language Model

```bash
# Wait for Ollama to start (check: docker logs ollama)
docker exec -it ollama ollama pull phi3:mini

# Verify
docker exec -it ollama ollama list
```

#### 4. Add Documents

```bash
# Copy PDFs to documents folder
cp /path/to/your/*.pdf backend/data/documents/

# Or use web UI to upload
```

#### 5. Initialize Vector Database

- Open browser: `http://localhost:5173`
- Navigate to **Admin Panel**
- Click **"ğŸ”„ Initialize DB"**
- Wait for indexing

#### 6. Start Querying!

**5 minutes from clone to first query.**

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚
â”‚  Interface  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastAPI Backend                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚     LangGraph RAG Agent        â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚  1. Query Analysis       â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  2. Query Rewriting (3x) â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  3. Hybrid Retrieval     â”‚  â”‚    â”‚
â”‚  â”‚  â”‚     - Vector Search      â”‚  â”‚    â”‚
â”‚  â”‚  â”‚     - BM25 Search        â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  4. RRF Fusion           â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  5. Cross-Encoder Rerank â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  6. LLM Generation       â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  7. Answer Validation    â”‚  â”‚    â”‚
â”‚  â”‚  â”‚  8. Self-Correction â†»    â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ChromaDBâ”‚  â”‚  Redis  â”‚  â”‚ SQLite â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technical Stack

**Backend (All Local)**:
- **LLM**: Phi-3 Mini (3.8B) via Ollama
- **Framework**: LangGraph for agent orchestration
- **Vector DB**: ChromaDB with persistent storage
- **Embeddings**: sentence-transformers/all-MiniLM-L6-v2 (offline)
- **Re-ranker**: cross-encoder/ms-marco-MiniLM-L-6-v2 (offline)
- **Keyword Search**: BM25 with Reciprocal Rank Fusion
- **Cache**: Redis (local, in-memory)
- **API**: FastAPI with async/await

**Frontend (Local Web App)**:
- React 18 + Vite
- Modern CSS with gradient design
- Axios with streaming support

**Infrastructure**:
- Docker Compose with GPU support
- SQLite for metrics/history/feedback (local)
- Localhost only - no external connections

---

## ğŸŒŸ Features

### Core Capabilities

| Feature | Description |
|---------|-------------|
| **ğŸ”„ Self-Correction** | LangGraph validation + automatic retry |
| **ğŸ“ Query Rewriting** | 3x query variations for comprehensive retrieval |
| **ğŸ” Hybrid Search** | Vector + BM25 with RRF fusion |
| **ğŸ¯ Cross-Encoder Re-ranking** | State-of-the-art precision |
| **âš¡ Streaming Responses** | Real-time ChatGPT-style generation |
| **ğŸ“Š RAGAS Evaluation** | Faithfulness, relevancy, recall metrics |
| **ğŸ’¾ Redis Caching** | Sub-second repeat queries |
| **ğŸ—‚ï¸ Chat History** | Persistent SQLite storage |

### ğŸ”’ Privacy-First & Offline

**100% Local Operation**:
- âœ… Your data never leaves your machine
- âœ… No external API calls (after model download)
- âœ… GDPR/HIPAA friendly
- âœ… Zero recurring costs

**Perfect For**:
- ğŸ¥ Healthcare: HIPAA-compliant patient data
- âš–ï¸ Legal: Confidential case documents
- ğŸ”¬ Research: Proprietary papers
- ğŸ¢ Enterprise: Internal knowledge bases

---

## ğŸ“Š System Performance

| Metric | Value | Impact |
|--------|-------|--------|
| **Self-Correction Rate** | 11.1% | Automatic quality improvement |
| **Cache Hit Rate** | 26.7% | Sub-second repeat queries |
| **P95 Latency** | 29.4s (Quality mode) | Accuracy over speed |
| **Faithfulness** | 50-83% | Answer grounding verified |
| **Error Rate** | 0.0% | Production-stable |

*(Based on 45 queries across Harry Potter corpus)*

---

## ğŸ“¸ Screenshots

### Main Chat Interface
<img width="1919" alt="Chat Interface" src="https://github.com/user-attachments/assets/8bfe81f7-9305-45fe-84c3-b059da75b989" />

*Response modes, chat history, document panel, streaming answers*

### Performance Metrics
<img width="1919" alt="Admin Dashboard" src="https://github.com/user-attachments/assets/0905390a-2834-4910-9e24-287a51eca498" />

*Real-time monitoring: latency, corrections, cache hits*

### RAGAS Evaluation
<img width="1496" alt="Evaluation Results" src="https://github.com/user-attachments/assets/2c1cc14a-6f2a-40bc-b63b-a1148b7983ed" />

*Automated quality: faithfulness 50%, relevancy 83%, recall 50%*

### ğŸ‘¥ User Feedback System

<img width="1793" height="522" alt="feedbacks" src="https://github.com/user-attachments/assets/ef33f264-b762-4fac-802f-37e4d475cd51" />

*Built-in feedback loop enables you to track answer quality from the user's perspective and identify areas for improvement.*


---

## ğŸ® Response Modes

### ğŸ¯ Quality Mode (Self-Correcting) - **Recommended**

1. Multi-query rewriting (3x variations)
2. Hybrid retrieval (vector + BM25)
3. RRF fusion
4. Cross-encoder re-ranking
5. LLM generation
6. Answer validation
7. Self-correction if invalid (up to 2 retries)

**Latency**: ~10.8s | **Best for**: Critical queries

### âš¡ Fast Mode (Streaming)

1. Single query (no rewriting)
2. Vector search only
3. No re-ranking
4. Token-by-token streaming

**Latency**: ~1.2s first token | **Best for**: Exploratory Q&A

### ğŸ’¬ Direct LLM (No Documents)

Pure LLM response without retrieval.

**Latency**: ~0.5s | **Best for**: General knowledge

---


## âš ï¸ Known Limitations

**Hallucinations**: Like all local LLMs (Phi-3 Mini, 3.8B params), occasional hallucinations may occur.

**Mitigation strategies**:
- Self-validation catches 11.1% of issues automatically
- Source attribution enables manual verification
- RAGAS faithfulness metric tracks answer quality
- Upgrade to larger models.

**Recommendation**: Always verify critical answers against sources.



## ğŸ”¬ Technical Deep Dive

### Self-Correction Mechanism

```python
def validate_answer(state: AgentState) -> dict:
    """LangGraph node: validates answer against context"""
    answer = state["answer"]
    context = state["context"]
    
    is_valid = llm_validator(answer, context)
    
    if not is_valid and state["correction_attempts"] < MAX_ATTEMPTS:
        return {"needs_correction": True, "correction_attempts": +1}
    
    return {"needs_correction": False}
```

### Hybrid Search Pipeline

```python
def hybrid_search(query: str) -> List[Document]:
    # 1. Multi-query rewriting
    queries = rewrite_query(query, n=3)
    
    # 2. Parallel retrieval
    vector_results = chromadb.search(queries, n=20)  # Semantic
    bm25_results = bm25.search(queries, n=10)        # Keyword
    
    # 3. Reciprocal Rank Fusion
    fused = rrf_fusion(vector_results, bm25_results)
    
    # 4. Cross-encoder re-ranking
    reranked = reranker.rerank(query, fused, top_k=5)
    
    return reranked
```

---

## ğŸ› ï¸ Configuration

**Key Parameters** (`backend/config.py`):

```python
# Model Configuration
OLLAMA_MODEL = "phi3:mini"
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
RERANKER_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"

# RAG Parameters
CHUNK_SIZE = 512
CHUNK_OVERLAP = 50
TOP_K_RETRIEVAL = 20
TOP_K_RERANK = 5
TOP_K_BM25 = 10

# Agent Parameters
MAX_CORRECTION_ATTEMPTS = 2
NUM_QUERY_VARIATIONS = 3
TEMPERATURE = 0.7

# Cache
CACHE_TTL = 3600
ENABLE_CACHE = True
```

---

## ğŸ“Š API Examples

### Query with Self-Correction

```bash
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Who is Voldemort?",
    "mode": "quality",
    "max_sentences": 6
  }'
```

**Response:**
```json
{
  "answer": "Voldemort, also known as Lord Voldemort...",
  "sources": [
    {
      "content": "...",
      "metadata": {"source": "harry_potter.pdf", "page": 1},
      "relevance_score": 0.95
    }
  ],
  "metadata": {
    "query_time": 10.8,
    "num_sources": 5,
    "correction_applied": false
  }
}
```

### Streaming Response

```bash
curl -X POST http://localhost:8000/api/query-stream \
  -d '{"query": "Explain Horcruxes"}' \
  --no-buffer
```

### Upload Document

```bash
curl -X POST http://localhost:8000/api/upload \
  -F "file=@document.pdf"
```

### Get Metrics

```bash
curl http://localhost:8000/api/metrics
```

**Interactive Docs**: `http://localhost:8000/docs`

---


## ğŸ§ª Evaluation
### Run Evaluation (auto-generates test questions from your documents)
```bash
curl -X POST "http://localhost:8000/api/evaluation/run?n_questions=50"
```
**Metrics**:
- **Faithfulness**: Answer accuracy vs context (target >80%)
- **Relevancy**: How well answer addresses question (target >80%)
- **Recall**: Retrieval quality (target >80%)

### Improving Low Scores

**Low Faithfulness (<70%)**:
- Increase `TOP_K_RETRIEVAL`
- Adjust `CHUNK_SIZE`
- Review prompt engineering

**Low Relevancy (<70%)**:
- Lower `TEMPERATURE`
- Improve query rewriting

**Low Recall (<70%)**:
- Add more documents
- Increase `TOP_K_BM25`

---

## ğŸ“ Project Structure

```
celeby-agentic-rag/
â”œâ”€â”€ ğŸ“„ docker-compose.yml        # Multi-service orchestration
â”œâ”€â”€ ğŸ“„ LICENSE                   # Apache 2.0
â”œâ”€â”€ ğŸ“„ README.md                
â”‚
â”œâ”€â”€ ğŸ“‚ backend/
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile
â”‚   â”œâ”€â”€ ğŸ“„ requirements.txt
â”‚   â”œâ”€â”€ ğŸ“„ config.py             # Centralized configuration
â”‚   â”œâ”€â”€ ğŸ“„ main.py               # FastAPI application entry
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ api/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ routes.py         # API endpoint definitions
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ services/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ agent.py          # â­ LangGraph RAG agent
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ llm.py            # Ollama LLM integration
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ embedding.py      # Document embeddings
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ vector_store.py   # ChromaDB + persistence
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ bm25_search.py    # BM25 keyword search
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ reranker.py       # Cross-encoder re-ranking
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ query_analyzer.py # Query understanding
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ cache.py          # Redis caching layer
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ metrics.py        # Performance tracking
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ chat_history.py   # Conversation persistence
â”‚   â”‚   â””â”€â”€ ğŸ“„ feedback.py       # User feedback storage
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ evaluation/
â”‚   â”‚   â””â”€â”€ ğŸ“„ ragas_eval.py     # RAGAS evaluation engine
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ models/
â”‚   â”‚   â””â”€â”€ ğŸ“„ __init__.py       # Pydantic models
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”‚   â””â”€â”€ ğŸ“‚ documents/        # Upload PDFs here
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ chroma_db/            # Persistent vector database
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“Š Database Files
â”‚       â”œâ”€â”€ chat_history.db      # Conversation logs
â”‚       â”œâ”€â”€ feedback.db          # User ratings
â”‚       â””â”€â”€ metrics.db           # Performance data
â”‚
â”œâ”€â”€ ğŸ“‚ frontend/
   â”œâ”€â”€ ğŸ“„ Dockerfile
   â”œâ”€â”€ ğŸ“„ package.json
   â”œâ”€â”€ ğŸ“„ vite.config.js
   â”œâ”€â”€ ğŸ“„ index.html
   â”‚
   â””â”€â”€ ğŸ“‚ src/
       â”œâ”€â”€ ğŸ“„ App.jsx           # Main app component
       â”œâ”€â”€ ğŸ“„ App.css
       â”œâ”€â”€ ğŸ“„ main.jsx
       â”œâ”€â”€ ğŸ“„ index.css
       â”‚
       â””â”€â”€ ğŸ“‚ components/
           â”œâ”€â”€ ğŸ“„ ChatInterface.jsx       # Main chat UI
           â”œâ”€â”€ ğŸ“„ ChatSidebar.jsx         # Chat history
           â”œâ”€â”€ ğŸ“„ DocumentsList.jsx       # Document manager
           â”œâ”€â”€ ğŸ“„ AdminPanel.jsx          # Metrics dashboard
           â”œâ”€â”€ ğŸ“„ MetricsDashboard.jsx    # Performance charts
           â””â”€â”€ ğŸ“„ (corresponding .css files)


    
```

---

## ğŸ› Troubleshooting

### Ollama Not Responding

```bash
docker logs ollama
docker restart ollama
docker exec -it ollama ollama list
```

### GPU Not Detected

**NVIDIA:**
```bash
nvidia-smi
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
```

**AMD:**
```bash
rocm-smi
docker run --rm --device=/dev/kfd --device=/dev/dri rocm/pytorch:latest rocm-smi
```

### ChromaDB Errors

```bash
docker-compose down -v
rm -rf backend/chroma_db/*
docker-compose up -d
```

### Slow Responses

```python
# config.py
ENABLE_CACHE = True
TOP_K_RETRIEVAL = 10  # Reduce from 20
TOP_K_RERANK = 3      # Reduce from 5
```

Or use Fast mode in UI.

---


## ğŸ“„ License

Apache License 2.0 - see [LICENSE](LICENSE)

---

## ğŸ™ Acknowledgments

LangChain â€¢ LangGraph â€¢ Ollama â€¢ ChromaDB â€¢ RAGAS â€¢ Sentence Transformers â€¢ FastAPI â€¢ React

---

## ğŸ“§ Contact

**Muhammed Eren Ã‡elebi**

- GitHub: [@merenceleby](https://github.com/merenceleby)
- LinkedIn: [linkedin.com/in/merencelebi](https://linkedin.com/in/merencelebi)
- Email: muhammederencelebii@gmail.com

---

<div align="center">

**â­ Star this repository if you find it useful!**

Made by Celeby

</div>
